# Logback

Logback is one of the most widely used logging frameworks in the Java Community. Itâ€™s a replacement for its predecessor, Log4j. Logback offers a faster implementation, provides more options for configuration, and more flexibility in archiving old log files.

## Why do we need logging?
Logging records events that occur during software execution. As users execute programs on the client side, the system accumulates log entries for support teams. In general, it allows for developers to access information about applications to which we do not have direct access. Without logs, we would have no idea of knowing what went wrong when an application crashes, or track and monitor application performance.

Also, a logging framework like Logback is critical because it allows us to use various logging levels and configure a threshold to determine which messages will be recorded in the application log files.

## Logback Logging Levels
Below are the Logback log levels, in order of least to most restrictive:
1. **ALL** => all levels
2. **DEBUG** => designates fine-grained informational events that are most useful to debug an application
3. **INFO** => informational messages that highlight the progress of the application at the coarse grained level
4. **WARN** => designates potentially harmful situations
5. **ERROR** => designates error events that might still allow the application to continue running
6. **FATAL** => severe error events that presumably lead the application to abort
7. **OFF** => highest possible level, intended to turn off logging

## How do logging levels work?
A log request of level *x* in a logger with level *y* is enabled with *x >= y*. For the standard levels, we have that 
    ALL < DEBUG < INFO < WARN < ERROR < FATAL < OFF


# Java Threads

## Intro to Concurrency
**Concurrency** refers to breaking up a task or piece of computation into different parts that can be executed independently, out of order, or in partial order without affecting the final outcome. One way - but not the only way - of achieving concurrency is by using multiple threads in the same program.

### Multi-core Processing
Most computers these days have multiple cores or CPUs, which means that calculations at the hardware level can be done in parallel. Without multiple cores, operating systems can still achieve concurrency with a process called **time splicing** - this means running one process for a short time, then switching to another, and back very rapidly. This ensures that no process or application is completely blocked.

On multi-core systems, different processes can be run on different CPUs entirely. This enables true parallelization and is a key benefit of writing multithreaded programs.

## Introduction to Threads
A thread is a subset of a process that is also an independent sequence of execution, but threads of the main process run in the same memory space, managed independently by a scheduler. So, we can think of a thread as a "path of execution", but they can access the same objects in memory.

Every thread that is created in a program is given its own call stack, where it stores local variables references. However, all threads share the same heap, where the objects live in memory. Thus, two threads could have separate variable references on two different stacks that still point to the same object in the heap.

### Multithreading
Multithreading extends the idea of multitasking into applications where you can subdivide operations in a single application into individual, parallel threads. Each thread can have its own task that it performs. The OS divides processing time not just with applications, but between threads. Multi-core processors can actually run multiple different processes and threads concurrently, enabling true parallelization.

In Java, multithreading is achieved via the [`Thread`](https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html) class and/or the [`Runnable`](https://docs.oracle.com/javase/8/docs/api/java/lang/Runnable.html) interface.

#### A Note on Best Practices
In general, it is best to avoid implementing multithreading yourself if possible. The benefit of multithreaded applications is better performance due to non-blocking execution. However, you should always measure or attempt to estimate the performance benefit you will get by using threads versus the tradeoff in complexity and subtle bugs that might be generated. Usually there are frameworks, tools, or libraries that have implemented the problem you are trying to solve, and you can leverage those instead of trying to build your own solution. For example, web servers like Apache Tomcat have multithreading built-in and provide APIs for dealing with network requests without having to worry about threads.

### Thread methods
A few important methods in the `Thread` class include:
* getters and setters for id, name, and priority
* `interrupt()` to explicitly interrupt the thread
* `isAlive()`, `isInterrupted()` and `isDaemon()` to test the state of the thread
* `join()` to wait for the thread to finish execution
* `start()` to actually begin thread execution after instantiation

A few important `static` methods are also defined:
* `Thread.currentThread()` which returns the thread that is currently executing
* `Thread.sleep(long millis)` which causes the currently executing thread to temporarily stop for a specified number of milliseconds

## Lifecycle of a Thread
At any given time, a thread can be in one of these states:
1. **New**: newly created thread that has not started executing
2. **Runnable**: either running or ready for execution but waiting for its resource allocation
3. **Blocked**: waiting to acquire a monitor lock to enter or re-enter a synchronized block/method
4. **Waiting**: waiting for some other thread to perform an action without any time limit
5. **Timed_Waiting**: waiting for some other thread to perform a specific action for a specified time period
6. **Terminated**: has completed its execution

<img src = "https://miro.medium.com/max/1197/1*AVdsesDdmzZz4XoKi-AHWQ.png" width = "600">

## Thread Priorities
Priorities signify which order threads are to be run. The Thread class contains a few `static` variables for priority:
* MIN_PRIORITY = 1
* NORM_PRIORITY = 5, default
* MAX_PRIORITY = 10


### Creating Threads
There are two options to create and execute a `Thread` in Java:
1. Create a class that implements the `Runnable` functional interface
* implement the `run()` method
* pass an instance of your class to a `Thread` constructor
* call the `start()` method on the thread
	
```java
	public class MyRunnable implements Runnable {
		@Override
		public void run() {
			System.out.println("Inside the MyRunnable class");
		}
	}
```

2. Create a class that extends `Thread`
* implement the `run()` method
* instantiate your class
* call the `start()` method
	
```java
	public class MyThread extends Thread {
		@Override
		public void run() {
			System.out.println("Inside the MyThread class");
		}
	}
```

```java
	public class ThreadDemo {
		public static void main(String[] args) {
			Thread myRunnable = new Thread(new MyRunnable());
			Thread myThread = new MyThread();
			myRunnable.start();
			myThread.start();
		}
	}
```

### Runnable and Lambda Expressions
Because `Runnable` is a *functional* interface, we can use a lambda expression to define thread behavior inline instead of implementing the interface in a separate class. We pass a lambda expression as the `Runnable` type required in the `Thread` constructor. For example:

```java
public class ThreadLambda {
  public static main(String[] args) {
    Thread willRun = new Thread(() -> {
	  System.out.println("Running!");
	});
	willRun.start();
  }
}
```

## Deadlock
The term "deadlock" describes a situation where 2 or more threads are blocked trying to access the same resource, waiting for the other. Neither thread can continue execution, so the program halts indefinitely.

### `synchronized` keyword
In a multithreaded environment, a race condition occurs when 2 or more threads attempt to access the same resource. Using the `synchronized` keyword on a piece of logic enforces that only one thread can access the resource at any given time. `synchronized` blocks or methods can be created using the keyword. Also, one way a class can be "thread-safe" is if all of its methods are `synchronized`.

## Producer-Consumer Problem

The Producer-Consumer problem is a classic example of a multi-process synchronization problem. Here, we have  a *fixed-size buffer* and two classes of threads - *producers* and *consumers*. Producers produces the data to the queue and Consumers consume the data from the queue. Both producer and consumer shares the same fixed-size buffer as a queue.

**Problem** - The producer should produce data only when the queue is not full. If the queue is full, then the producer shouldn't be allowed to put any data into the queue. The consumer should consume data only when the queue is not empty. If the queue is empty, then the consumer shouldn't be allowed to take any data from the queue.

We can solve the Producer-Consumer problem by using `wait()` & `notify()`methods to communicate between producer and consumer threads. The `wait()` method to pause the producer or consumer thread depending on the queue size. The `notify()` method sends a notification to the waiting thread.

Producer thread will keep on producing data for Consumer to consume. It will use `wait()` method when Queue is full and use `notify()` method to send notification to Consumer thread once data is added to the queue.

Consumer thread will consume the data form the queue. It will also use `wait()` method to wait if queue is empty. It will also use `notify()` method to send notification to producer thread after consuming data from the queue.

# Algorithms 

## What is an Algorithm?

An algorithm is a process, or set of rules to be followed in calculations or some other problem-solving operations. Typically, the size of the input will often affect the overall runtime of the algorithm.

There are many different types but we'll be covering 3 today:

- Brute Force Search
- Binary Search
- Recursive

## Complexity of Algorithms

### Time Complexity

Time complexity is typically talked about much more than Space complexity simply because we live in a day and age when we want everything to be fast, but more often than not, care a bit less about how much space a solution takes up. When calculating and comparing time complexities, as mentioned, we really only care about the worst possible case for a given solution, that is, what is the absolute most time, or iterations, that this given operation will take or perform.

### Space Complexity

When calculating the space complexity for a given solution, we often ask the question "How much space does this solution need for its computation", or rather, how much memory needs to be reserved or allocated during its execution.

### Big O Notation

Complexity is typically denoted with what we call "Big O Notation", which looks something like O(n) or O(1) where the variable within the parentheses denotes the upper bound worst case for a given implementation.

#### Big O Cheat Sheet

| O(1)          | O(log n)                 | O(n)                              | O(n log n)             | O(n^2)                          | O(2^n)                             | O(n!)          |
| ------------- | ------------------------ | --------------------------------- | ---------------------- | ------------------------------- | ---------------------------------- | -------------- |
| Constant Time | Logarithmic Time         | Linear Time                       | Asymptotic Time        | Quadratic Time                  | Fibonacci Time                     | Factorial Time |
| Best Case     | Slightly Worse than O(1) | Dependent on number of iterations | Worse than Linear Time | Much Worse than Asymptotic Time | Growth Doubles with Each New Input | Worst Case     |

<img src="https://miro.medium.com/max/1400/1*5ZLci3SuR0zM_QlZOADv8Q.jpeg" width = "600">

## Searching Algorithms

- **Basic Linear Search (Brute Force)** - O(n) Linear
- **Binary Search** - O(log n) Logarithmic
  > *Logarithmic running time (O(log n)) essentially means that the running time grows in proportion to the logarithm of the input size - as an example, if 10 items takes at most some amount of time x, and 100 items takes at most, say, 2x, and 10,000 items takes at most 4x, then it's looking like an O(log n) time complexity.*
<br>

## Binary Search
Imagine that you have a shelf of books and you are looking for a book authored by *Shakespear*.  With Linear search, you would start from the beginning and check each book.  The faster approach (O(log n)) would be by using **Binary Search**.

> The collection must be **sorted** to apply Binary Search
- Imagine that the books are **sorted by last name**.
- With Binary Search you would **first located the center of the book shelf**.
- Imagine the book in the center of the shelf is authored by someone with a last name starting with "U".

<br>

> [ P Q R S T **U** V W X Y Z]
<br>

- Since "S" is *less than* "U", we can eliminate the entire right half of the book shelf, only searching between "P" - "T".

<br>

> [ P Q R S T ~~**U** V W X Y Z~~]
<br>

- We repeat this process, continuously finding the midpoint and checking whether our target value is greater than or less than, until we find "S".

<br>

> [ P Q **R** S T] <br>
> [ ~~P Q **R**~~ S T] <br> 
> [ **S** ~~T~~]
<br>

Binary Search on a *sorted* array looks like this:

<br>

```java
public static int binarySearch(int[] arr, int x) {
    int min = 0;
    int max = arr.length - 1;
    while (min <= max) {
        
        int mid = (min + max) / 2;
        
        if (x < arr[mid]) { // if the target value is less than
            max = mid - 1; // the midpoint, we discard the RHS
        } else if (x > arr[mid]) {
            min = mid + 1; // otherwise we discard the LHS
        } else {
            return mid;
        }
    }
    return -1;
}
```

<br>

### Logarithmic Complexity O(log n)
Logarithm is the inverse of exponentiation.  The way this works is by division - the size of the input is repeatedly being divided by 2. Note the radicall difference in number of operations performed based on input between Linear O(n) vs Binary Search O(log n).

| Binary Search |            |
|---------------|------------|
| Input Size(n) | # of Steps |
| 10            | 4          |
| 100           | 7          |
| 1,000         | 10         |
| 10,000        | 14         |
| 100,000       | 17         |
| 1,000,000     | 20         |
| 10,000,000    | 24         |
| 100,000,000   | 27         |
| 1,000,000,000 | 30         |

The formula to get the logarithm of a number is `log`<sub>`2`</sub>`n`.  For example, let's say we had an input size of **10,000**.  To find how many opearations this would take with Binary Search to find a target value, in a calculator you would run:

> `n` = 10,000 <br>
> `log`<sub>`2`</sub>`n` = **14** <br>
Finding the `log`<sub>`2`</sub> of a number, is equivelent to asking "how many times can I divide this by 2 until I reach 1?".  That's why the `log`<sub>`2`</sub> of 10,000 is 14.


## Sorting

I don't have a lot of notes of sorting algorithms but the two we want to cover are Bubble Sort and Merge Sort. I'll post some resources here so y'all can view them and learn

### Bubble Sort

Idea here is that the max values _bubbles_ its way to the top and we repeat this until we've sorted the entire array

Resources:

- https://youtu.be/xli_FI7CuzA?si=p-8BZSlfXcITfspx

- https://www.geeksforgeeks.org/bubble-sort/

- https://www.youtube.com/watch?v=Cq7SMsQBEUw


### Merge Sort

Idea here is to break up the array into smaller arrays until you get to the smallest arrays possible and sort all of those, then _merge_ all the sorted arrays until we get to the full array again (you'll need to sort again on the way back up).

Resources:

- https://www.youtube.com/watch?v=4VqmGXwpLqc

- https://www.geeksforgeeks.org/merge-sort/#

- https://www.youtube.com/watch?v=ZRPoEKHXTJg



